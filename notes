- bullet point
* to do

NOTES:
- use vector database for efficient storage and retrieval of data
- for testing model:
	- difficulty with complex choices for answers
		i.e. a) N + S
	- answer must be in either character file or town file
	* incorporate database in future
* vet information and determine if new or not
- dynamic ID
* test different embeddings (all-mpnet-base-v2 vs all-MiniLM-L6-v2)

-----------------------------------
- semantic role labeling
	- go from phrase to topic
- 4-5 more topics
	- fact based
- Topological Data Analysis
	- https://web.archive.org/web/20220712055205id_/https://dl.acm.org/doi/pdf/10.1145/3477495.3531881
	- http://bigdataieee.org/BigData2020/files/IEEE_BigData_2020_Tutorial5_TDA_Tutorial.pdf
	- Dataset: https://cims.nyu.edu/~sbowman/multinli/
- Contradictory event pairs
	- https://aclanthology.org/W15-0813.pdf
- Contradiciton for rumours
	- https://aclanthology.org/W16-5004.pdf
	- PHEME RTE dataset: https://www.pheme.eu/2016/04/12/pheme-rte-dataset/
- Finding Contradicitons in Text (2008)
	- https://aclanthology.org/P08-1118.pdf

PROBLEMS
- context pool saturated -> need to weed out bad responses
	- contrasting information
		- local alignment of tokens
	- fact or opinion
	- confidence score
- negation
- data storage for efficient training

1. determine if talking about same thing
2. determine if contrasting
3. do something (confidence score, etc.)
-----------------------------------
Paper flow:
Intro
    Introduce project and motivation
Related Works
    Language Modelling
        Language models (LMs) are simply models that assign a probability to a sequence of words \cite{NLP Book}.
            N-Gram
                Simplest LM is an n-gram
                Unigram -> one word
                Bigram -> two words
                Trigram-> three words
                etc.
                Trained on text, finds all sequences of tokens of length $n-1$, uses probability to calculate next word based on provided sequence
                example
            Neural Network -> train, pass a single token or sentence through, embedding is values from final layer
                More complex than N-Gram, more computational power
                A collection of nodes, or ``neurons'', connected by weights
                Trained on a large amount of text
                Post-training, passing a token or a sequence of tokens into the network, and retrieving the weights of the final layer of the network (NOT the output, the layer before that) provide the embedding for the inputted text
                Revolutionary one is BERT (bidirectional encoder transformers), transformer architecture, trained on \_ tokens using the Masked Language Modeling (MLM) approach.
                MLM masks words at any part of a sentence, model is trained ot predict which word is hidden based on its training

    Embeddings
        word
        context/sentence
    Generated text
        SotA models
    LLM Identity?
    Prompt Engineering
    Dialogue Systems?
Challenges
    Hallucination
    Over-Saturated Database

Building the Dialogue System
    Model selection
        LLaMA
        GPT-J
        GPT 3.5
        Final Decision
    Knowledge Base
        Relational Database
        Vector Database

-------------------------------
Hallucinating example
can you provide me references for an author that discusses AI in dialogue systems
chat online -> Hallucinated a book that exists but incorrect authors
after asking it to verify actual book
-------------------------------
+++ 	Linguistics Papers	+++
+ Contradticiton Detection in Financial Reports +
	- Additional pre-training (part-of-speech tagging) to enhance semantic capabilties (spaCy framework)
	- XLM-RoBERTa (most general model) performs best when pretrained for POS tagging and fine tuned on SNLI and finCD datasets
	- Architecture: 
		* encoder + feed-forward network
	- pre-training on finacial docs may not help with contradiction detection
	- future work, include more info as context for transformer
	
+ Factual Inconsistency Classification +
	- 4 model pipeline
		* Input: Claim, Context
		* Claim, Context > M1 > Inconsistent claim triples <S, R, T>, inconsistnet context span
		* Claim, Context, Inconsistent claim triples <S, R, T>, inconsistent context span > M2 > Inconsistency type, Inconsistent claim component
		* Inconsistent context span, inconsistnet claim component > M3 > coarse Inconsistent entity type
		* Inconsistent context span, inconsistnet claim component, coarse Inconsistent entity type > M4 > Fine-grained inconsistent entity-type
		
+ Contradiction-Specific Word Embeddings +
	- Contradiction occurs hwen sentencews are unliekly to be correct at the same time
	- PROBLEM: Contradicting words and sentences are very close in vector space
	- CONTRIBUTION: Data constructino method to generate large-scale corpus for contradiction specific word embeddings (CWE)
	- CONTRIBUTION: Neural net for CWE
	- CONTRIBUTION: apply CWE in smeantic relation representation to detect contradiction (+6.11% above SotA)
	- CWE Learning Model:
		* 5 layers: lookup > linear > tanh > sum > linear
		* Input: pair of sentences
		* solve classificatino problem through ranking objective function
	- 
	









































